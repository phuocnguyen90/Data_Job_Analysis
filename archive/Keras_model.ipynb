{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# For visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# For modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Set TF logger to only print errors (dismiss warnings)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the files were copied correctly and look like we expect them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\dataset\\data_job_new_with_predicted_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>YOE</th>\n",
       "      <th>Location</th>\n",
       "      <th>Est_Salary</th>\n",
       "      <th>Job_Description</th>\n",
       "      <th>Link</th>\n",
       "      <th>Min_level</th>\n",
       "      <th>Max_level</th>\n",
       "      <th>...</th>\n",
       "      <th>Data_Scientist_prob</th>\n",
       "      <th>Business_Analyst_prob</th>\n",
       "      <th>Business_Intelligence_prob</th>\n",
       "      <th>Others_prob</th>\n",
       "      <th>Data_Engineer</th>\n",
       "      <th>Data_Analyst</th>\n",
       "      <th>Data_Scientist</th>\n",
       "      <th>Business_Analyst</th>\n",
       "      <th>Business_Intelligence</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3617</td>\n",
       "      <td>FIDT</td>\n",
       "      <td>( Data ) Research Intern</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HCM</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Minimum 10 hours/week; Students complete at le...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/3631700502</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.320370e-04</td>\n",
       "      <td>1.420000e-05</td>\n",
       "      <td>4.020000e-09</td>\n",
       "      <td>8.928759e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1084</td>\n",
       "      <td>Kyanon Digital</td>\n",
       "      <td>Machine Learning Engineer Intern</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HCM</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Familiar with Object-Oriented Programming, Mod...</td>\n",
       "      <td>https://glints.com/vn/opportunities/jobs/machi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.200000e-10</td>\n",
       "      <td>2.540000e-07</td>\n",
       "      <td>1.490000e-07</td>\n",
       "      <td>9.947981e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2526</td>\n",
       "      <td>Paditech</td>\n",
       "      <td>Blockchain Intern</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3rd, 4th, 5th year students or have graduated ...</td>\n",
       "      <td>https://www.topcv.vn/viec-lam/thuc-tap-sinh-bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.848903e-03</td>\n",
       "      <td>2.628318e-03</td>\n",
       "      <td>2.480000e-05</td>\n",
       "      <td>9.846249e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2858</td>\n",
       "      <td>Maico Group</td>\n",
       "      <td>BI Intern</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HCM</td>\n",
       "      <td>85.0</td>\n",
       "      <td>The program will not be suitable for you who a...</td>\n",
       "      <td>https://www.careerlink.vn/tim-viec-lam/thuc-ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.765148e-03</td>\n",
       "      <td>7.810333e-02</td>\n",
       "      <td>2.572874e-03</td>\n",
       "      <td>9.400327e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3423</td>\n",
       "      <td>DataGenius</td>\n",
       "      <td>Data Engineer | Data Analyst | AI Engineer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HCM</td>\n",
       "      <td>105.0</td>\n",
       "      <td>DataGenius company is recruiting Data Engineer...</td>\n",
       "      <td>https://www.facebook.com/groups/datanalyticsvn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.051680e-04</td>\n",
       "      <td>7.030000e-07</td>\n",
       "      <td>3.560000e-09</td>\n",
       "      <td>5.590000e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID         Company                                   Job_Title  YOE  \\\n",
       "0  3617            FIDT                    ( Data ) Research Intern  0.0   \n",
       "1  1084  Kyanon Digital            Machine Learning Engineer Intern  0.0   \n",
       "2  2526        Paditech                           Blockchain Intern  0.0   \n",
       "3  2858     Maico Group                                   BI Intern  0.0   \n",
       "4  3423      DataGenius  Data Engineer | Data Analyst | AI Engineer  0.0   \n",
       "\n",
       "  Location  Est_Salary                                    Job_Description  \\\n",
       "0      HCM        65.0  Minimum 10 hours/week; Students complete at le...   \n",
       "1      HCM        85.0  Familiar with Object-Oriented Programming, Mod...   \n",
       "2       HN        85.0  3rd, 4th, 5th year students or have graduated ...   \n",
       "3      HCM        85.0  The program will not be suitable for you who a...   \n",
       "4      HCM       105.0  DataGenius company is recruiting Data Engineer...   \n",
       "\n",
       "                                                Link  Min_level  Max_level  \\\n",
       "0      https://www.linkedin.com/jobs/view/3631700502          0          0   \n",
       "1  https://glints.com/vn/opportunities/jobs/machi...          0          0   \n",
       "2  https://www.topcv.vn/viec-lam/thuc-tap-sinh-bl...          0          0   \n",
       "3  https://www.careerlink.vn/tim-viec-lam/thuc-ta...          0          0   \n",
       "4  https://www.facebook.com/groups/datanalyticsvn...          0          0   \n",
       "\n",
       "   ...  Data_Scientist_prob  Business_Analyst_prob  \\\n",
       "0  ...         3.320370e-04           1.420000e-05   \n",
       "1  ...         4.200000e-10           2.540000e-07   \n",
       "2  ...         3.848903e-03           2.628318e-03   \n",
       "3  ...         2.765148e-03           7.810333e-02   \n",
       "4  ...         2.051680e-04           7.030000e-07   \n",
       "\n",
       "   Business_Intelligence_prob   Others_prob  Data_Engineer  Data_Analyst  \\\n",
       "0                4.020000e-09  8.928759e-01              0             0   \n",
       "1                1.490000e-07  9.947981e-01              0             0   \n",
       "2                2.480000e-05  9.846249e-01              0             0   \n",
       "3                2.572874e-03  9.400327e-01              0             0   \n",
       "4                3.560000e-09  5.590000e-10              1             1   \n",
       "\n",
       "   Data_Scientist  Business_Analyst  Business_Intelligence  Others  \n",
       "0               0                 0                      0       1  \n",
       "1               0                 0                      0       1  \n",
       "2               0                 0                      0       1  \n",
       "3               0                 0                      0       1  \n",
       "4               0                 0                      0       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3631 entries, 0 to 3630\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   YOE                    2974 non-null   float64\n",
      " 1   Location               3621 non-null   object \n",
      " 2   Est_Salary             3377 non-null   float64\n",
      " 3   Job_Description        3631 non-null   object \n",
      " 4   Min_level              3631 non-null   int64  \n",
      " 5   Max_level              3631 non-null   int64  \n",
      " 6   VN                     3631 non-null   int64  \n",
      " 7   Overseas               3631 non-null   int64  \n",
      " 8   Remote                 3631 non-null   int64  \n",
      " 9   Data_Engineer          3631 non-null   int64  \n",
      " 10  Data_Analyst           3631 non-null   int64  \n",
      " 11  Data_Scientist         3631 non-null   int64  \n",
      " 12  Business_Analyst       3631 non-null   int64  \n",
      " 13  Business_Intelligence  3631 non-null   int64  \n",
      " 14  Others                 3631 non-null   int64  \n",
      "dtypes: float64(2), int64(11), object(2)\n",
      "memory usage: 425.6+ KB\n"
     ]
    }
   ],
   "source": [
    "col_to_drop = ['Job_Title','ID', 'Company', 'Link', 'Data_Engineer_prob','Data_Analyst_prob', 'Data_Scientist_prob','Business_Analyst_prob','Business_Intelligence_prob','Others_prob']\n",
    "clean_df=df.drop(columns=col_to_drop)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3631 entries, 0 to 3630\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   YOE                    3631 non-null   float64\n",
      " 1   Location               3631 non-null   object \n",
      " 2   Est_Salary             3631 non-null   float64\n",
      " 3   Job_Description        3631 non-null   object \n",
      " 4   Min_level              3631 non-null   int64  \n",
      " 5   Max_level              3631 non-null   int64  \n",
      " 6   VN                     3631 non-null   int64  \n",
      " 7   Overseas               3631 non-null   int64  \n",
      " 8   Remote                 3631 non-null   int64  \n",
      " 9   Data_Engineer          3631 non-null   int64  \n",
      " 10  Data_Analyst           3631 non-null   int64  \n",
      " 11  Data_Scientist         3631 non-null   int64  \n",
      " 12  Business_Analyst       3631 non-null   int64  \n",
      " 13  Business_Intelligence  3631 non-null   int64  \n",
      " 14  Others                 3631 non-null   int64  \n",
      "dtypes: float64(2), int64(11), object(2)\n",
      "memory usage: 425.6+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.dropna(subset='Est_Salary')\n",
    "clean_df.fillna(value=0, inplace=True)\n",
    "\n",
    "job_categories = ['Data_Engineer', 'Data_Analyst', 'Data_Scientist', 'Business_Analyst', 'Business_Intelligence','Others']\n",
    "# Calculate the average YOE for each job category and round it\n",
    "average_yoe_by_category = clean_df.groupby(job_categories)['YOE'].mean().round().reset_index()\n",
    "\n",
    "# Fill in the missing 'YOE' values based on the job category\n",
    "for _, row in average_yoe_by_category.iterrows():\n",
    "    condition = (clean_df[job_categories] == tuple(row[job_categories])).all(axis=1)\n",
    "    clean_df.loc[condition, 'YOE'] = row['YOE']\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an input pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which column is the target\n",
    "LABEL_COLUMN = 'Est_Salary'\n",
    "\n",
    "# Specify numerical columns\n",
    "# Note you should create another list with STRING_COLS if you\n",
    "# had text data but in this case all features are numerical\n",
    "NUMERIC_COLS = ['YOE', 'Min_level',\n",
    "                'Max_level', 'VN',\n",
    "                'Overseas', 'Remote', 'dayofweek'\n",
    "                'Data_Engineer','Data_Analyst',\t'Data_Scientist','Business_Analyst','Business_Intelligence','Others']\n",
    "\n",
    "CATEGORICAL_COLS  = ['Location']\n",
    "\n",
    "\n",
    "# A function to separate features and labels\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a CSV file\n",
    "def load_dataset(path, batch_size=1, mode='eval'):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(path, batch_size)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels)  # features, label\n",
    "    if mode == 'train':\n",
    "        # Notice the repeat method is used so this dataset will loop infinitely\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "        # take advantage of multi-threading; 1=AUTOTUNE\n",
    "        dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DNN Model in Keras\n",
    "\n",
    "Since the model is defined using `feature columns` the first layer might look different to what you are used to. This is done by declaring two dictionaries, one for the inputs (defined as Input layers) and one for the features (defined as feature columns).\n",
    "\n",
    "Then computing the `DenseFeatures` tensor by passing in the feature columns to the constructor of the `DenseFeatures` layer and passing in the inputs to the resulting tensor (this is easier to understand with code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model():\n",
    "    # Define input layers for numeric features\n",
    "    numeric_inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Define input layer for categorical feature\n",
    "    categorical_inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in CATEGORICAL_COLS\n",
    "    }\n",
    "\n",
    "    # Define feature columns for numeric features\n",
    "    numeric_feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Define feature column for categorical feature\n",
    "    categorical_feature_columns = {\n",
    "        colname: fc.embedding_column(\n",
    "            fc.categorical_column_with_vocabulary_list(colname, vocabulary_list=clean_df[\"Location\"].unique()),\n",
    "            dimension=8  # Specify the embedding dimension\n",
    "        )\n",
    "        for colname in CATEGORICAL_COLS\n",
    "    }\n",
    "     \n",
    "\n",
    "    # Construct DenseFeatures for numeric features\n",
    "    numeric_dnn_inputs = layers.DenseFeatures(numeric_feature_columns.values())(numeric_inputs)\n",
    "\n",
    "    # Construct DenseFeatures for categorical features\n",
    "    categorical_dnn_inputs = layers.DenseFeatures(categorical_feature_columns.values())(categorical_inputs)\n",
    "\n",
    "    # Concatenate numeric and categorical features\n",
    "    concatenated_inputs = layers.concatenate([numeric_dnn_inputs, categorical_dnn_inputs])\n",
    "\n",
    "    # Two hidden layers of 32 and 8 units, respectively\n",
    "    h1 = layers.Dense(32, activation='relu', name='h1')(concatenated_inputs)\n",
    "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
    "\n",
    "    # Final output is a linear activation because this is a regression problem\n",
    "    output = layers.Dense(1, activation='linear', name='output')(h2)\n",
    "\n",
    "    # Create model with inputs and output\n",
    "    model = models.Model({**numeric_inputs, **categorical_inputs}, output)\n",
    "\n",
    "    # Compile model (Mean Squared Error is suitable for regression)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "                      'mse'\n",
    "                  ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build our DNN model and inspect the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"dense_features_5\" (type DenseFeatures).\n\nCan't convert Python sequence with mixed types to Tensor.\n\nCall arguments received by layer \"dense_features_5\" (type DenseFeatures):\n  • features={'Location': 'tf.Tensor(shape=(None,), dtype=string)'}\n  • cols_to_output_tensors=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\git\\Data_Job_Analysis\\archive\\Keras_model.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Save compiled model into a variable\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m build_dnn_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Plot the layer architecture and relationship between input features\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mplot_model(model, \u001b[39m'\u001b[39m\u001b[39mdnn_model.png\u001b[39m\u001b[39m'\u001b[39m, show_shapes\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, rankdir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\PC\\git\\Data_Job_Analysis\\archive\\Keras_model.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m numeric_dnn_inputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDenseFeatures(numeric_feature_columns\u001b[39m.\u001b[39mvalues())(numeric_inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Construct DenseFeatures for categorical features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m categorical_dnn_inputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mDenseFeatures(categorical_feature_columns\u001b[39m.\u001b[39;49mvalues())(categorical_inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Concatenate numeric and categorical features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/git/Data_Job_Analysis/archive/Keras_model.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m concatenated_inputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mconcatenate([numeric_dnn_inputs, categorical_dnn_inputs])\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\data_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\PC\\miniconda3\\envs\\data_venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"dense_features_5\" (type DenseFeatures).\n\nCan't convert Python sequence with mixed types to Tensor.\n\nCall arguments received by layer \"dense_features_5\" (type DenseFeatures):\n  • features={'Location': 'tf.Tensor(shape=(None,), dtype=string)'}\n  • cols_to_output_tensors=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Save compiled model into a variable\n",
    "model = build_dnn_model()\n",
    "\n",
    "# Plot the layer architecture and relationship between input features\n",
    "tf.keras.utils.plot_model(model, 'dnn_model.png', show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model architecture defined it is time to train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "You are going to train the model for 20 epochs using a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = len(pd.read_csv('/tmp/data/taxi-train.csv'))\n",
    "NUM_EVAL_EXAMPLES = len(pd.read_csv('/tmp/data/taxi-valid.csv'))\n",
    "\n",
    "print(f\"training split has {NUM_TRAIN_EXAMPLES} examples\\n\")\n",
    "print(f\"evaluation split has {NUM_EVAL_EXAMPLES} examples\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously defined function to load the datasets from the original csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "trainds = load_dataset('/tmp/data/taxi-train*', TRAIN_BATCH_SIZE, 'train')\n",
    "\n",
    "# Evaluation dataset\n",
    "evalds = load_dataset('/tmp/data/taxi-valid*', 1000, 'eval').take(NUM_EVAL_EXAMPLES//1000)\n",
    "\n",
    "# Needs to be specified since the dataset is infinite\n",
    "# This happens because the repeat method was used when creating the dataset\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // TRAIN_BATCH_SIZE\n",
    "\n",
    "# Train the model and save the history\n",
    "history = model.fit(trainds,\n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training curves\n",
    "\n",
    "Now lets visualize the training history of the model with the raw features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting metrics for a given history\n",
    "def plot_curves(history, metrics):\n",
    "    nrows = 1\n",
    "    ncols = 2\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for idx, key in enumerate(metrics):\n",
    "        ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        plt.plot(history.history[key])\n",
    "        plt.plot(history.history[f'val_{key}'])\n",
    "        plt.title(f'model {key}')\n",
    "        plt.ylabel(key)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "\n",
    "# Plot history metrics\n",
    "plot_curves(history, ['loss', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training history doesn't look very promising showing an erratic behaviour. Looks like the training process struggled to transverse the high dimensional space that the current features create.\n",
    "\n",
    "Nevertheless let's use it for prediction.\n",
    "\n",
    "Notice that the latitude and longitude values should revolve around (`37`, `45`) and (`-70`, `-78`) respectively since these are the range of coordinates for New York city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a taxi ride (a data point)\n",
    "taxi_ride = {\n",
    "    'pickup_longitude': tf.convert_to_tensor([-73.982683]),\n",
    "    'pickup_latitude': tf.convert_to_tensor([40.742104]),\n",
    "    'dropoff_longitude': tf.convert_to_tensor([-73.983766]),\n",
    "    'dropoff_latitude': tf.convert_to_tensor([40.755174]),\n",
    "    'passenger_count': tf.convert_to_tensor([3.0]),\n",
    "    'hourofday': tf.convert_to_tensor([3.0]),\n",
    "    'dayofweek': tf.convert_to_tensor([3.0]),\n",
    "}\n",
    "\n",
    "# Use the model to predict\n",
    "prediction = model.predict(taxi_ride, steps=1)\n",
    "\n",
    "# Print prediction\n",
    "print(f\"the model predicted a fare total of {float(prediction):.2f} USD for the ride.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted this particular ride to be around 12 USD. However you know the model performance is not the best as it was showcased by the training history. Let's improve it by using **Feature Engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Performance Using Feature Engineering\n",
    "\n",
    "Going forward you will only use geo-spatial features as these are the most relevant when calculating the fare since this value is mostly dependant on the distance transversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dayofweek and hourofday features\n",
    "NUMERIC_COLS = ['pickup_longitude', 'pickup_latitude',\n",
    "                'dropoff_longitude', 'dropoff_latitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are dealing exclusively with geospatial data you will create some transformations that are aware of this geospatial nature. This help the model make a better representation of the problem at hand.\n",
    "\n",
    "For instance the model cannot magically understand what a coordinate is supposed to represent and since the data is taken from New York only, the latitude and longitude revolve around (`37`, `45`) and (`-70`, `-78`) respectively, which is arbitrary for the model. A good first step is to scale these values.\n",
    "\n",
    "**Notice all transformations are created by defining functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_longitude(lon_column):\n",
    "    return (lon_column + 78)/8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_latitude(lat_column):\n",
    "    return (lat_column - 37)/8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important fact is that the fare of a taxi ride is proportional to the distance of the ride. But as the features currently are, there is no way for the model to infer that the pair of (`pickup_latitude`, `pickup_longitude`) represent the point where the passenger started the ride and the pair (`dropoff_latitude`, `dropoff_longitude`) represent the point where the ride ended. More importantly, the model is not aware that the distance between these two points is crucial for predicting the fare.\n",
    "\n",
    "To solve this, a new feature (which is a transformation of the other ones) that provides this information is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff*londiff + latdiff*latdiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying transformations\n",
    "\n",
    "Now you will define the `transform` function which will apply the previously defined transformation functions. To apply the actual transformations you will be using `Lambda` layers apply a function to values (in this case the inputs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(inputs, numeric_cols):\n",
    "\n",
    "    # Make a copy of the inputs to apply the transformations to\n",
    "    transformed = inputs.copy()\n",
    "\n",
    "    # Define feature columns\n",
    "    feature_columns = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in numeric_cols\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = layers.Lambda(\n",
    "            scale_longitude,\n",
    "            name=f\"scale_{lon_col}\")(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = layers.Lambda(\n",
    "            scale_latitude,\n",
    "            name=f'scale_{lat_col}')(inputs[lat_col])\n",
    "\n",
    "    # add Euclidean distance\n",
    "    transformed['euclidean'] = layers.Lambda(\n",
    "        euclidean,\n",
    "        name='euclidean')([inputs['pickup_longitude'],\n",
    "                           inputs['pickup_latitude'],\n",
    "                           inputs['dropoff_longitude'],\n",
    "                           inputs['dropoff_latitude']])\n",
    "\n",
    "\n",
    "    # Add euclidean distance to feature columns\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "\n",
    "    return transformed, feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the model\n",
    "\n",
    "Next, you'll create the DNN model now with the engineered (transformed) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model():\n",
    "\n",
    "    # input layer (notice type of float32 since features are numeric)\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # transformed features\n",
    "    transformed, feature_columns = transform(inputs, numeric_cols=NUMERIC_COLS)\n",
    "\n",
    "    # Constructor for DenseFeatures takes a list of numeric columns\n",
    "    # and the resulting tensor takes a dictionary of Lambda layers\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "\n",
    "    # two hidden layers of 32 and 8 units, respectively\n",
    "    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
    "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
    "\n",
    "    # final output is a linear activation because this is a regression problem\n",
    "    output = layers.Dense(1, activation='linear', name='fare')(h2)\n",
    "\n",
    "    # Create model with inputs and output\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    # Compile model (Mean Squared Error is suitable for regression)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compiled model into a variable\n",
    "model = build_dnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model architecture has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the layer architecture and relationship between input features\n",
    "tf.keras.utils.plot_model(model, 'dnn_model_engineered.png', show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is very useful for understanding the relationships and dependencies between the original and the transformed features!\n",
    "\n",
    "**Notice that the input of the model now consists of 5 features instead of the original 7, thus reducing the dimensionality of the problem.**\n",
    "\n",
    "Let's now train the model that includes feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and save the history\n",
    "history = model.fit(trainds,\n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features `passenger_count`, `hourofday` and `dayofweek` were excluded since they were omitted when defining the input pipeline.\n",
    "\n",
    "Now lets visualize the training history of the model with the engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history metrics\n",
    "plot_curves(history, ['loss', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot better than the previous training history! Now the loss and error metrics are decreasing with each epoch and both curves (train and validation) are very close to each other. Nice job!\n",
    "\n",
    "Let's do a prediction with this new model on the example we previously used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict\n",
    "prediction = model.predict(taxi_ride, steps=1)\n",
    "\n",
    "# Print prediction\n",
    "print(f\"the model predicted a fare total of {float(prediction):.2f} USD for the ride.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, now the model predicts a fare that is roughly half of what the previous model predicted! Looks like the model with the raw features was overestimating the fare by a great margin.\n",
    "\n",
    "Notice that you get a warning since the `taxi_ride` dictionary contains information about the unused features. You can supress it by redefining `taxi_ride` without these values but it is useful to know that Keras is smart enough to handle it on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this ungraded lab!** Now you should have a clearer understanding of the importance and impact of performing feature engineering on your data.\n",
    "\n",
    "This process is very domain-specific and requires a great understanding of the situation that is being modelled. Because of this, new techniques that switch from a manual to an automatic feature engineering have been developed and you will check some of them in an upcoming lab.\n",
    "\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
